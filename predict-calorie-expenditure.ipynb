{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d2b6be",
   "metadata": {},
   "source": [
    "# Best Performing notebook from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e046b54a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-09T16:51:08.282316Z",
     "iopub.status.busy": "2025-05-09T16:51:08.281917Z",
     "iopub.status.idle": "2025-05-09T16:51:10.523354Z",
     "shell.execute_reply": "2025-05-09T16:51:10.522074Z"
    },
    "papermill": {
     "duration": 2.247982,
     "end_time": "2025-05-09T16:51:10.525286",
     "exception": false,
     "start_time": "2025-05-09T16:51:08.277304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/playground-series-s5e5/sample_submission.csv\n",
      "/kaggle/input/playground-series-s5e5/train.csv\n",
      "/kaggle/input/playground-series-s5e5/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f9be38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:51:10.532027Z",
     "iopub.status.busy": "2025-05-09T16:51:10.531516Z",
     "iopub.status.idle": "2025-05-09T16:51:11.636593Z",
     "shell.execute_reply": "2025-05-09T16:51:11.635575Z"
    },
    "papermill": {
     "duration": 1.110341,
     "end_time": "2025-05-09T16:51:11.638395",
     "exception": false,
     "start_time": "2025-05-09T16:51:10.528054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\n",
    "\n",
    "# Weight_per_Age\n",
    "df['Weight_per_Age'] = df['Weight'] / (df['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "df['HeartRate_per_kg'] = df['Heart_Rate'] / df['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "df['Duration_per_age'] = df['Duration'] / (df['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "df['Duration_heart_rate']=df['Duration']*df['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "df['Duration_per_weight']=df['Duration']/df['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "df['duration_sum']=df['Duration_per_weight']+df['Duration_heart_rate']+df['Duration_per_age']\n",
    "df['duration_multi']=df['Duration_per_weight']*df['Duration_heart_rate']*df['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "df['BMI']=df['Weight']/((df['Height']/100) ** 2)\n",
    "df['BMI']=df['BMI'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c581dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T16:51:11.644583Z",
     "iopub.status.busy": "2025-05-09T16:51:11.644260Z",
     "iopub.status.idle": "2025-05-09T17:40:47.495716Z",
     "shell.execute_reply": "2025-05-09T17:40:47.494551Z"
    },
    "papermill": {
     "duration": 2975.85676,
     "end_time": "2025-05-09T17:40:47.497391",
     "exception": false,
     "start_time": "2025-05-09T16:51:11.640631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:51:15.535856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746809475.832443      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746809475.918509      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2400\n",
      "[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.111361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2401\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.111761\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2398\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.111138\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2399\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.112460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2399\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.111181\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2401\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 4.110267\n",
      "Optimized Stacking Ensemble RMSLE: 0.0030\n",
      "R-squared Score: 0.997138903718074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.preprocessing import StandardScaler,  OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"Calories\"])\n",
    "\n",
    "y = np.log(df['Calories'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "\n",
    "categorical_features = [\"Sex\"]\n",
    "\n",
    "\n",
    "# Create a ColumnTransformer to apply different preprocessing strategies\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "\n",
    "    ],\n",
    "\n",
    ").fit(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train = preprocessor.transform(X)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Define base models\n",
    "\n",
    "base_models = [\n",
    "\n",
    "    ('xgb', XGBRegressor(n_estimators=5000, learning_rate=0.061849546072614363, max_depth=5)),\n",
    "\n",
    "    ('lgbm', LGBMRegressor(n_estimators=3000, learning_rate=0.021849546072614363, max_depth=16)),\n",
    "\n",
    "    ('cat', CatBoostRegressor(learning_rate=0.06758463422, max_depth=11, iterations =370, verbose=0)),\n",
    "    \n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Define meta-model (aggregator)\n",
    "meta_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "\n",
    "\n",
    "# Build Stacking Ensemble\n",
    "\n",
    "stacking_ensemble = StackingRegressor(\n",
    "    estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "stacking_ensemble.fit(X_train, y)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred = stacking_ensemble.predict(X_test)\n",
    "\n",
    "\n",
    "rmsle_score = mean_squared_log_error(np.exp(y_test), np.exp(y_pred))\n",
    "print(f\"Optimized Stacking Ensemble RMSLE: {rmsle_score:.4f}\")\n",
    "r2 = r2_score(np.exp(y_test), np.exp(y_pred))\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a580d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T17:40:47.504480Z",
     "iopub.status.busy": "2025-05-09T17:40:47.504127Z",
     "iopub.status.idle": "2025-05-09T17:41:38.407032Z",
     "shell.execute_reply": "2025-05-09T17:41:38.405906Z"
    },
    "papermill": {
     "duration": 50.910123,
     "end_time": "2025-05-09T17:41:38.410350",
     "exception": false,
     "start_time": "2025-05-09T17:40:47.500227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>27.162572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750001</td>\n",
       "      <td>108.856540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750002</td>\n",
       "      <td>87.529183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750003</td>\n",
       "      <td>125.369641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750004</td>\n",
       "      <td>75.791915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>999995</td>\n",
       "      <td>25.912894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>999996</td>\n",
       "      <td>9.668233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>999997</td>\n",
       "      <td>73.055768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>999998</td>\n",
       "      <td>169.331192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>999999</td>\n",
       "      <td>77.605947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    Calories\n",
       "0       750000   27.162572\n",
       "1       750001  108.856540\n",
       "2       750002   87.529183\n",
       "3       750003  125.369641\n",
       "4       750004   75.791915\n",
       "...        ...         ...\n",
       "249995  999995   25.912894\n",
       "249996  999996    9.668233\n",
       "249997  999997   73.055768\n",
       "249998  999998  169.331192\n",
       "249999  999999   77.605947\n",
       "\n",
       "[250000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")\n",
    "\n",
    "# Weight_per_Age\n",
    "X_submission['Weight_per_Age'] = X_submission['Weight'] / (X_submission['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "X_submission['HeartRate_per_kg'] = X_submission['Heart_Rate'] / X_submission['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "X_submission['Duration_per_age'] = X_submission['Duration'] / (X_submission['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "X_submission['Duration_heart_rate']=X_submission['Duration']*X_submission['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "X_submission['Duration_per_weight']=X_submission['Duration']/X_submission['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "X_submission['duration_sum']=X_submission['Duration_per_weight']+X_submission['Duration_heart_rate']+X_submission['Duration_per_age']\n",
    "X_submission['duration_multi']=X_submission['Duration_per_weight']*X_submission['Duration_heart_rate']*X_submission['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "X_submission['BMI']=X_submission['Weight']/((X_submission['Height']/100) ** 2)\n",
    "X_submission['BMI']=X_submission['BMI'].round(2)\n",
    "\n",
    "out=X_submission[[\"id\"]]\n",
    "X_submission = preprocessor.transform(X_submission.drop(columns=[\"id\"]))\n",
    "#X_submission = poly.transform(X_submission)\n",
    "#X_submission = pca.transform(X_submission)\n",
    "y_out =  stacking_ensemble.predict(X_submission)\n",
    "out[\"Calories\"] = np.exp(y_out)\n",
    "out.to_csv(\"submission.csv\", index=False)\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3038.605248,
   "end_time": "2025-05-09T17:41:41.473193",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T16:51:02.867945",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
