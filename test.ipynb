{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\n",
    "from sklearn.preprocessing import StandardScaler,  OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "X=df.drop(columns=[\"id\",\"Calories\"])\n",
    "y=df['Calories']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28269b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "categorical_features = [\"Sex\"]\n",
    "\n",
    "# Create a ColumnTransformer to apply different preprocessing strategies\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "    ],\n",
    ").fit(X_train)\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "\n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "poly.fit(X_train)\n",
    "\n",
    "X_train = poly.transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Step 5: Define parameters for the LightGBM model\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 63,\n",
    "    'n_estimators': 100,\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# Step 6: Train the model\n",
    "model = lgb.train(params, train_data, \n",
    "                 valid_sets=[test_data],\n",
    "                 num_boost_round=200)\n",
    "\n",
    "# Step 7: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model's performance\n",
    "msle = mean_squared_log_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {msle}\")\n",
    "print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "sorted_indices = np.argsort(feature_importance)[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c368f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0, 10),\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=200)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Return RMSE for optimization\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Run 50 trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with optimized hyperparameters\n",
    "model_optimized = lgb.train(best_params, lgb.Dataset(X_train, label=y_train),\n",
    "                            valid_sets=[lgb.Dataset(X_test, label=y_test)], num_boost_round=200)\n",
    "\n",
    "# Make final predictions\n",
    "y_pred_final = model_optimized.predict(X_test)\n",
    "\n",
    "# Evaluate final model\n",
    "mse_final = mean_squared_error(y_test, y_pred_final)\n",
    "r2_final = r2_score(y_test, y_pred_final)\n",
    "\n",
    "print(f\"Optimized Mean Squared Error: {mse_final}\")\n",
    "print(f\"Optimized R-squared Score: {r2_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30092eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = pd.read_csv(\"data/test.csv\")\n",
    "out=X_submission[[\"id\"]]\n",
    "X_submission = preprocessor.transform(X_submission.drop(columns=[\"id\"]))\n",
    "X_submission = poly.transform(X_submission)\n",
    "y_out = model_optimized.predict(X_submission)\n",
    "out[\"Calories\"] = y_out\n",
    "out.to_csv(\"data/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6730449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "\n",
    "# Weight_per_Age\n",
    "df['Weight_per_Age'] = df['Weight'] / (df['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "df['HeartRate_per_kg'] = df['Heart_Rate'] / df['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "df['Duration_per_age'] = df['Duration'] / (df['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "df['Duration_heart_rate']=df['Duration']*df['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "df['Duration_per_weight']=df['Duration']/df['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "df['duration_sum']=df['Duration_per_weight']+df['Duration_heart_rate']+df['Duration_per_age']\n",
    "df['duration_multi']=df['Duration_per_weight']*df['Duration_heart_rate']*df['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "df['BMI']=df['Weight']/(df['Height'] ** 2)\n",
    "df['BMI']=df['BMI'].round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"Calories\"])\n",
    "\n",
    "y = np.log(df['Calories'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "\n",
    "categorical_features = [\"Sex\"]\n",
    "\n",
    "\n",
    "# Create a ColumnTransformer to apply different preprocessing strategies\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "\n",
    "    ],\n",
    "\n",
    ").fit(X_train)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(2, interaction_only=True)\n",
    "\n",
    "\n",
    "X_train = preprocessor.transform(X_train)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "#poly.fit(X_train)\n",
    "\n",
    "\n",
    "#X_train = poly.transform(X_train)\n",
    "\n",
    "#X_test = poly.transform(X_test)\n",
    "\n",
    "\n",
    "#pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "#X_train = pca.fit_transform(X_train)\n",
    "#X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "'''\n",
    "# Define Autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 35  # Dimensionality of compressed features\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train Autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32,\n",
    "                validation_data=(X_test, X_test), verbose=1)\n",
    "\n",
    "# Extract compressed features\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "X_train = encoder.predict(X_train)\n",
    "X_test = encoder.predict(X_test)\n",
    "'''\n",
    "\n",
    "# Define base models\n",
    "\n",
    "base_models = [\n",
    "\n",
    "    ('xgb', XGBRegressor(n_estimators=5000, learning_rate=0.061849546072614363, max_depth=5)),\n",
    "\n",
    "    ('lgbm', LGBMRegressor(n_estimators=3000, learning_rate=0.021849546072614363, max_depth=16)),\n",
    "\n",
    "    ('cat', CatBoostRegressor(learning_rate=0.06758463422, max_depth=11, iterations =370, verbose=0)),\n",
    "    \n",
    "    ('rf', RandomForestRegressor(n_estimators=1000, max_depth=10, random_state=42))\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Define Neural Network meta-model\n",
    "#meta_model = MLPRegressor(hidden_layer_sizes=(\n",
    "#    128, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "# Define meta-model (aggregator)\n",
    "meta_model = Ridge()\n",
    "\n",
    "\n",
    "# Build Stacking Ensemble\n",
    "\n",
    "stacking_ensemble = StackingRegressor(\n",
    "    estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred = stacking_ensemble.predict(X_test)\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "rmsle_score = mean_squared_log_error(np.exp(y_test), y_pred)\n",
    "print(f\"Optimized Stacking Ensemble RMSLE: {rmsle_score:.4f}\")\n",
    "r2 = r2_score(np.exp(y_test), y_pred)\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60208597",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Weight_per_Age\n",
    "X_submission['Weight_per_Age'] = X_submission['Weight'] / (X_submission['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "X_submission['HeartRate_per_kg'] = X_submission['Heart_Rate'] / X_submission['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "X_submission['Duration_per_age'] = X_submission['Duration'] / (X_submission['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "X_submission['Duration_heart_rate']=X_submission['Duration']*X_submission['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "X_submission['Duration_per_weight']=X_submission['Duration']/X_submission['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "X_submission['duration_sum']=X_submission['Duration_per_weight']+X_submission['Duration_heart_rate']+X_submission['Duration_per_age']\n",
    "X_submission['duration_multi']=X_submission['Duration_per_weight']*X_submission['Duration_heart_rate']*X_submission['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "X_submission['BMI']=X_submission['Weight']/(X_submission['Height'] ** 2)\n",
    "X_submission['BMI']=X_submission['BMI'].round(2)\n",
    "\n",
    "\n",
    "out=X_submission[[\"id\"]]\n",
    "X_submission = preprocessor.transform(X_submission.drop(columns=[\"id\"]))\n",
    "X_submission = poly.transform(X_submission)\n",
    "y_out =  stacking_ensemble.predict(X_submission)\n",
    "out[\"Calories\"] = y_out\n",
    "out.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "\n",
    "# Weight_per_Age\n",
    "df['Weight_per_Age'] = df['Weight'] / (df['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "df['HeartRate_per_kg'] = df['Heart_Rate'] / df['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "df['Duration_per_age'] = df['Duration'] / (df['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "df['Duration_heart_rate']=df['Duration']*df['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "df['Duration_per_weight']=df['Duration']/df['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "df['duration_sum']=df['Duration_per_weight']+df['Duration_heart_rate']+df['Duration_per_age']\n",
    "df['duration_multi']=df['Duration_per_weight']*df['Duration_heart_rate']*df['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "df['BMI']=df['Weight']/(df['Height'] ** 2)\n",
    "df['BMI']=df['BMI'].round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"Calories\"])\n",
    "\n",
    "y = np.log(df['Calories'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "\n",
    "categorical_features = [\"Sex\"]\n",
    "\n",
    "\n",
    "# Create a ColumnTransformer to apply different preprocessing strategies\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "\n",
    "    ],\n",
    "\n",
    ").fit(X_train)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(2, interaction_only=True)\n",
    "\n",
    "\n",
    "X_train = preprocessor.transform(X_train)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Define base models\n",
    "\n",
    "base_models = [\n",
    "\n",
    "    ('xgb', XGBRegressor(n_estimators=5000, learning_rate=0.061849546072614363, max_depth=5)),\n",
    "\n",
    "    ('lgbm', LGBMRegressor(n_estimators=3000, learning_rate=0.021849546072614363, max_depth=16)),\n",
    "\n",
    "    ('cat', CatBoostRegressor(learning_rate=0.06758463422, max_depth=11, iterations =370, verbose=0))\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def mlp_evaluate(hidden_layer_1, hidden_layer_2, alpha):\n",
    "    meta_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(int(hidden_layer_1), int(hidden_layer_2)),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=alpha,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    stacking_ensemble = StackingRegressor(\n",
    "        estimators=base_models, final_estimator=meta_model\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(stacking_ensemble, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Define parameter bounds for Bayesian Optimization\n",
    "param_bounds = {\n",
    "    'hidden_layer_1': (16, 256),  # Neurons in first layer\n",
    "    'hidden_layer_2': (8, 128),  # Neurons in second layer\n",
    "    'alpha': (0.0001, 0.1)        # Regularization strength\n",
    "}\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=mlp_evaluate, pbounds=param_bounds, random_state=42)\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters for MLPRegressor:\", optimizer.max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1334c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = optimizer.max['params']\n",
    "optimized_meta_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(int(best_params['hidden_layer_1']), int(best_params['hidden_layer_2'])),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=best_params['alpha'],\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Rebuild the stacking ensemble with optimized MLPRegressor\n",
    "optimized_stacking_ensemble = StackingRegressor(\n",
    "    estimators=base_models, final_estimator=optimized_meta_model\n",
    ")\n",
    "\n",
    "\n",
    "optimized_stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = optimized_stacking_ensemble.predict(X_test)\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "rmsle_score = mean_squared_log_error(np.exp(y_test), y_pred)\n",
    "print(f\"Optimized Stacking Ensemble RMSLE: {rmsle_score:.4f}\")\n",
    "r2 = r2_score(np.exp(y_test), y_pred)\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "import gc\n",
    "from scipy.stats import uniform, randint\n",
    "import time\n",
    "\n",
    "# Custom RMSLE scorer\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Logarithmic Error\"\"\"\n",
    "    y_pred = np.maximum(y_pred, 1e-5)\n",
    "    y_true = np.maximum(y_true, 1e-5)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "class GPUAcceleratedStackingEnsemble(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"GPU-accelerated stacking ensemble that efficiently utilizes GPU resources\"\"\"\n",
    "    def __init__(self, meta_model=None, n_folds=5, random_state=42, gpu_id=0):\n",
    "        self.meta_model = meta_model if meta_model else ElasticNet(random_state=random_state)\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.gpu_id = gpu_id\n",
    "        \n",
    "        # Will be initialized later\n",
    "        self.xgb_model = None\n",
    "        self.lgb_model = None\n",
    "        self.ctb_model = None\n",
    "        \n",
    "        self.base_models = []\n",
    "        self.base_models_trained = []\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        # Initialize with GPU settings\n",
    "        self.xgb_model = xgb.XGBRegressor(\n",
    "            objective='reg:squaredlogerror', \n",
    "            random_state=self.random_state,\n",
    "            tree_method='gpu_hist',  # GPU acceleration\n",
    "            gpu_id=self.gpu_id\n",
    "        )\n",
    "        \n",
    "        self.lgb_model = lgb.LGBMRegressor(\n",
    "            objective='regression', \n",
    "            random_state=self.random_state,\n",
    "            device='gpu',  # GPU acceleration\n",
    "            gpu_platform_id=0,\n",
    "            gpu_device_id=self.gpu_id\n",
    "        )\n",
    "        \n",
    "        self.ctb_model = ctb.CatBoostRegressor(\n",
    "            loss_function='RMSE', \n",
    "            random_state=self.random_state,\n",
    "            verbose=0,\n",
    "            task_type='GPU',  # GPU acceleration\n",
    "            devices=f'{self.gpu_id}'\n",
    "        )\n",
    "        \n",
    "        self.base_models = [self.xgb_model, self.lgb_model, self.ctb_model]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(self.base_models) == 0:\n",
    "            self._initialize_models()\n",
    "            \n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            start_time = time.time()\n",
    "            print(f\"Training model {i+1}/{len(self.base_models)} ({model.__class__.__name__})\")\n",
    "            \n",
    "            # Generate out-of-fold predictions\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                \n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X_train, y_train)\n",
    "                meta_features[val_idx, i] = clone_model.predict(X_val)\n",
    "                \n",
    "                del clone_model\n",
    "                gc.collect()\n",
    "            \n",
    "            # Fit on full dataset\n",
    "            model_copy = clone(model)\n",
    "            model_copy.fit(X, y)\n",
    "            self.base_models_trained.append(model_copy)\n",
    "            \n",
    "            print(f\"Model {i+1} training completed in {time.time() - start_time:.2f} seconds\")\n",
    "            gc.collect()\n",
    "        \n",
    "        # Train meta-model\n",
    "        self.meta_model.fit(meta_features, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            model.predict(X) for model in self.base_models_trained\n",
    "        ])\n",
    "        return self.meta_model.predict(meta_features)\n",
    "\n",
    "def gpu_accelerated_tuning(X_train, y_train, X_test=None, y_test=None, cv=5, gpu_id=0):\n",
    "    \"\"\"\n",
    "    GPU-accelerated hyperparameter tuning with automated batch size selection\n",
    "    and memory management for optimal performance\n",
    "    \"\"\"\n",
    "    best_models = []\n",
    "    \n",
    "    # Calculate appropriate batch sizes based on dataset size and available GPU memory\n",
    "    # These are examples and should be adjusted based on your GPU memory\n",
    "    data_size_mb = X_train.nbytes / (1024 * 1024)\n",
    "    print(f\"Dataset size: {data_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Parameter distributions\n",
    "    xgb_param_dist = {\n",
    "        'max_depth': randint(5, 30),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'n_estimators': randint(500, 3000),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda': uniform(0, 5),\n",
    "        # GPU-specific parameters\n",
    "        'max_bin': randint(128, 512),  # Controls GPU memory usage\n",
    "        'gpu_id': [gpu_id]\n",
    "    }\n",
    "    \n",
    "    lgb_param_dist = {\n",
    "        'num_leaves': randint(20, 100),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'n_estimators': randint(500, 3000),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'reg_alpha': uniform(0, 1),\n",
    "        'reg_lambda': uniform(0, 1),\n",
    "        # GPU-specific parameters\n",
    "        'device': ['gpu'],\n",
    "        'gpu_platform_id': [0],\n",
    "        'gpu_device_id': [gpu_id],\n",
    "        'max_bin': randint(128, 255)  # Controls GPU memory usage\n",
    "    }\n",
    "    \n",
    "    ctb_param_dist = {\n",
    "        'depth': randint(8, 16),\n",
    "        'learning_rate': uniform(0.01, 0.5),\n",
    "        'iterations': randint(500, 3000),\n",
    "        'l2_leaf_reg': uniform(4, 16),\n",
    "        # GPU-specific parameters\n",
    "        'task_type': ['GPU'],\n",
    "        'devices': [f'{gpu_id}'],\n",
    "        'gpu_ram_part': uniform(0.3, 0.7)  # Portion of GPU memory to use\n",
    "    }\n",
    "    \n",
    "    # Tune XGBoost with GPU\n",
    "    print(\"Tuning XGBoost with GPU acceleration...\")\n",
    "    start_time = time.time()\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squaredlogerror', \n",
    "        random_state=42,\n",
    "        tree_method='gpu_hist',  # GPU algorithm\n",
    "        gpu_id=gpu_id\n",
    "    )\n",
    "    \n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=xgb_param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=rmsle_scorer,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1  # Use 1 for GPU to avoid conflicts\n",
    "    )\n",
    "    \n",
    "    xgb_search.fit(X_train, y_train)\n",
    "    print(f\"Best XGBoost RMSLE: {-xgb_search.best_score_:.5f}\")\n",
    "    print(f\"Best XGBoost params: {xgb_search.best_params_}\")\n",
    "    print(f\"XGBoost tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "    best_models.append(('xgb', xgb_search.best_estimator_))\n",
    "    \n",
    "    best_xgb = clone(xgb_search.best_estimator_)\n",
    "    del xgb_search, xgb_model\n",
    "    gc.collect()\n",
    "    \n",
    "    # Tune LightGBM with GPU\n",
    "    print(\"\\nTuning LightGBM with GPU acceleration...\")\n",
    "    start_time = time.time()\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective='regression', \n",
    "        random_state=42,\n",
    "        device='gpu',\n",
    "        gpu_platform_id=0,\n",
    "        gpu_device_id=gpu_id,\n",
    "        verbose = 0\n",
    "    )\n",
    "    \n",
    "    lgb_search = RandomizedSearchCV(\n",
    "        estimator=lgb_model,\n",
    "        param_distributions=lgb_param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=rmsle_scorer,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1  # Use 1 for GPU to avoid conflicts\n",
    "    )\n",
    "    \n",
    "    lgb_search.fit(X_train, y_train)\n",
    "    print(f\"Best LightGBM RMSLE: {-lgb_search.best_score_:.5f}\")\n",
    "    print(f\"Best LightGBM params: {lgb_search.best_params_}\")\n",
    "    print(f\"LightGBM tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "    best_models.append(('lgb', lgb_search.best_estimator_))\n",
    "    \n",
    "    best_lgb = clone(lgb_search.best_estimator_)\n",
    "    del lgb_search, lgb_model\n",
    "    gc.collect()\n",
    "    \n",
    "    # Tune CatBoost with GPU\n",
    "    print(\"\\nTuning CatBoost with GPU acceleration...\")\n",
    "    start_time = time.time()\n",
    "    ctb_model = ctb.CatBoostRegressor(\n",
    "        loss_function='RMSE', \n",
    "        random_state=42, \n",
    "        verbose=0,\n",
    "        task_type='GPU',\n",
    "        devices=f'{gpu_id}'\n",
    "    )\n",
    "    \n",
    "    ctb_search = RandomizedSearchCV(\n",
    "        estimator=ctb_model,\n",
    "        param_distributions=ctb_param_dist,\n",
    "        n_iter=20,\n",
    "        scoring=rmsle_scorer,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1  # Use 1 for GPU to avoid conflicts\n",
    "    )\n",
    "    \n",
    "    ctb_search.fit(X_train, y_train)\n",
    "    print(f\"Best CatBoost RMSLE: {-ctb_search.best_score_:.5f}\")\n",
    "    print(f\"Best CatBoost params: {ctb_search.best_params_}\")\n",
    "    print(f\"CatBoost tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "    best_models.append(('ctb', ctb_search.best_estimator_))\n",
    "    \n",
    "    best_ctb = clone(ctb_search.best_estimator_)\n",
    "    del ctb_search, ctb_model\n",
    "    gc.collect()\n",
    "    \n",
    "    # Now tune the meta-model (CPU-based since these are simple models)\n",
    "    print(\"\\nTuning Stacking Ensemble...\")\n",
    "    \n",
    "    meta_models = [\n",
    "        {\n",
    "            'name': 'ElasticNet',\n",
    "            'model': ElasticNet(random_state=42),\n",
    "            'param_dist': {\n",
    "                'alpha': uniform(0.0001, 1.0),\n",
    "                'l1_ratio': uniform(0.1, 0.8)\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Ridge',\n",
    "            'model': Ridge(random_state=42),\n",
    "            'param_dist': {\n",
    "                'alpha': uniform(0.0001, 10.0)\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Lasso',\n",
    "            'model': Lasso(random_state=42),\n",
    "            'param_dist': {\n",
    "                'alpha': uniform(0.0001, 1.0)\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_meta_model = None\n",
    "    best_ensemble = None\n",
    "    \n",
    "    for meta_info in meta_models:\n",
    "        print(f\"\\nTesting {meta_info['name']} as meta-model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        stacking = GPUAcceleratedStackingEnsemble(\n",
    "            meta_model=meta_info['model'],\n",
    "            n_folds=cv,\n",
    "            random_state=42,\n",
    "            gpu_id=gpu_id\n",
    "        )\n",
    "        \n",
    "        stacking._initialize_models()\n",
    "        stacking.base_models = [best_xgb, best_lgb, best_ctb]\n",
    "        \n",
    "        param_dist = {f'meta_model__{k}': v for k, v in meta_info['param_dist'].items()}\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=stacking,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=10,\n",
    "            scoring=rmsle_scorer,\n",
    "            cv=cv,\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=1  # Use 1 for GPU\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        current_score = -search.best_score_\n",
    "        \n",
    "        print(f\"{meta_info['name']} best RMSLE: {current_score:.5f}\")\n",
    "        print(f\"{meta_info['name']} best params: {search.best_params_}\")\n",
    "        print(f\"{meta_info['name']} tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_meta_model = meta_info['name']\n",
    "            best_ensemble = search.best_estimator_\n",
    "        \n",
    "        del search, stacking\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nBest Meta-Model: {best_meta_model}\")\n",
    "    print(f\"Best Stacking Ensemble RMSLE: {best_score:.5f}\")\n",
    "    \n",
    "    # Evaluate on test set if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        y_pred = best_ensemble.predict(X_test)\n",
    "        test_rmsle = rmsle(y_test, y_pred)\n",
    "        print(f\"Test RMSLE: {test_rmsle:.5f}\")\n",
    "        \n",
    "        for name, model in best_models:\n",
    "            y_pred = model.predict(X_test)\n",
    "            test_rmsle = rmsle(y_test, y_pred)\n",
    "            print(f\"{name} Test RMSLE: {test_rmsle:.5f}\")\n",
    "    \n",
    "    return best_ensemble, best_models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ee7f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 141.91 MB\n",
      "Tuning XGBoost with GPU acceleration...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:45:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:45:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:45:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [08:45:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:45:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:46:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:46:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:46:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:46:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:47:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:47:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:47:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:47:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:47:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:47:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:48:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:48:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:48:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:48:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:48:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:48:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:49:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:49:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:51:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:51:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:52:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:52:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:53:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:53:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:55:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:55:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:55:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:55:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:55:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:55:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:56:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:56:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:56:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:56:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:56:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:56:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:56:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:56:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:57:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:57:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:57:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:57:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:57:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:57:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:58:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:58:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:58:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:58:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:59:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:59:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:59:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:59:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:59:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:59:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:00:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:00:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:00:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:00:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:00:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:00:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:00:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:00:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:01:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:01:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:01:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:01:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:01:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:01:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:02:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:02:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:02:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:02:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:02:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:02:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:03:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:03:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:04:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:04:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:05:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:05:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:06:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:06:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:06:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:06:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:07:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:07:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:08:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:08:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:08:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:08:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:09:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:09:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:09:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:09:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:10:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:10:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:10:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:10:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:11:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:11:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:11:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:11:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:11:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:11:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:12:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:12:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:12:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:12:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [09:12:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:12:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     68\u001b[39m X_train = preprocessor.transform(X_train)\n\u001b[32m     70\u001b[39m X_test = preprocessor.transform(X_test)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m best_model, base_models = \u001b[43mgpu_accelerated_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mgpu_accelerated_tuning\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, cv, gpu_id)\u001b[39m\n\u001b[32m    163\u001b[39m xgb_model = xgb.XGBRegressor(\n\u001b[32m    164\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mreg:squaredlogerror\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    165\u001b[39m     random_state=\u001b[32m42\u001b[39m,\n\u001b[32m    166\u001b[39m     tree_method=\u001b[33m'\u001b[39m\u001b[33mgpu_hist\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# GPU algorithm\u001b[39;00m\n\u001b[32m    167\u001b[39m     gpu_id=gpu_id\n\u001b[32m    168\u001b[39m )\n\u001b[32m    170\u001b[39m xgb_search = RandomizedSearchCV(\n\u001b[32m    171\u001b[39m     estimator=xgb_model,\n\u001b[32m    172\u001b[39m     param_distributions=xgb_param_dist,\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m     n_jobs=\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Use 1 for GPU to avoid conflicts\u001b[39;00m\n\u001b[32m    179\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mxgb_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest XGBoost RMSLE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m-xgb_search.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest XGBoost params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_search.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    864\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    870\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nicholas\\anaconda3\\envs\\LLM\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "\n",
    "df['Weight_per_Age'] = df['Weight'] / (df['Age'] + 1)\n",
    "\n",
    "# HeartRate per Weight\n",
    "df['HeartRate_per_kg'] = df['Heart_Rate'] / df['Weight']\n",
    "\n",
    "# Duration Per Age\n",
    "df['Duration_per_age'] = df['Duration'] / (df['Age'] + 1)\n",
    "\n",
    "# Duration * Heart Rate\n",
    "df['Duration_heart_rate'] = df['Duration']*df['Heart_Rate']\n",
    "\n",
    "# Intensity\n",
    "df['Duration_per_weight'] = df['Duration']/df['Weight']\n",
    "\n",
    "# All Durations add and multi\n",
    "df['duration_sum'] = df['Duration_per_weight'] + \\\n",
    "    df['Duration_heart_rate']+df['Duration_per_age']\n",
    "df['duration_multi'] = df['Duration_per_weight'] * \\\n",
    "    df['Duration_heart_rate']*df['Duration_per_age']\n",
    "\n",
    "# Creating new column 'BMI'\n",
    "df['BMI'] = df['Weight']/(df['Height'] ** 2)\n",
    "df['BMI'] = df['BMI'].round(2)\n",
    "\n",
    "df['Body_Temp2'] = df['Body_Temp']**2\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"Calories\"])\n",
    "\n",
    "y = np.log(df['Calories'])\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "\n",
    "categorical_features = [\"Sex\"]\n",
    "X['Sex'] = X['Sex'].map({'female': 1, 'male': 0})\n",
    "\n",
    "for col in categorical_features:\n",
    "    for num_feature in numerical_features:\n",
    "        X[f'{num_feature}_x_{col}'] = X[num_feature] * X[col]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "numerical_features = [col for col in X.columns if col not in [\"Sex\"]]\n",
    "\n",
    "categorical_features = [\"Sex\"]\n",
    "\n",
    "\n",
    "# Create a ColumnTransformer to apply different preprocessing strategies\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "\n",
    "    ],\n",
    "    remainder = \"passthrough\"\n",
    "\n",
    ").fit(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train = preprocessor.transform(X_train)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "best_model, base_models = gpu_accelerated_tuning(X_train, y_train.to_numpy(), X_test, y_test.to_numpy(), gpu_id=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b69c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
