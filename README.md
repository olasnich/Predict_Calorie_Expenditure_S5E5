# Predict Calorie Expenditure

This repository contains code for predicting calorie expenditure, primarily developed for a Kaggle Playground Series competition (Season 5, Episode 5). The project focuses on machine learning techniques, including advanced feature engineering, hyperparameter tuning with GPU acceleration, and ensemble modeling to achieve high prediction accuracy.




## Project Structure

The project is organized into the following directories:

*   `data/`: This directory is intended to store the raw and processed datasets (`train.csv`, `test.csv`, `sample_submission.csv`). These files are not included in the repository and should be downloaded separately from the [Kaggle competition page](https://www.kaggle.com/c/playground-series-s5e5/data).
*   `src/`: Contains all the Python scripts for feature engineering, model training, and utility functions.
    *   `src/utils/`: Helper functions for feature engineering and hyperparameter tuning.
*   `notebooks/`: Jupyter notebooks used for data visualization, initial exploration, and submission generation.
*   `models/`: This directory will store trained machine learning models (e.g., `.pkl` files) after the training process.
*   `environment/`: Contains the `env.yaml` file for setting up the Conda environment with all necessary dependencies.




## Setup

To set up the development environment, follow these steps:

1.  **Clone the repository:**

2.  **Download the dataset:**

    Download `train.csv`, `test.csv`, and `sample_submission.csv` from the [Kaggle competition page](https://www.kaggle.com/c/playground-series-s5e5/data) and place them into the `data/` directory.

3.  **Create Conda environment:**

    Navigate to the `environment/` directory and create the Conda environment using the provided `env.yaml` file:

    ```bash
    conda env create -f environment/env.yaml
    conda activate calorie_predict
    ```

    This will install all the necessary Python packages, including `lightgbm`, `xgboost`, `catboost`, `tensorflow`, and `scikit-learn`, along with their GPU-enabled versions if your system supports it.




## Usage

This project provides scripts for training models, performing hyperparameter tuning, and generating submission files.

### 1. Training and Ensemble Prediction

The `train_model.py` script handles data loading, feature engineering, GPU-accelerated hyperparameter tuning, and training of a stacking ensemble model. It also generates a submission file.

To run the training and generate predictions:

```bash
python src/train_model.py
```

This script will:
*   Load `train.csv` and `test.csv` from the `data/` directory.
*   Apply feature engineering defined in `src/utils/feature_engineering.py`.
*   Perform GPU-accelerated hyperparameter tuning for XGBoost, LightGBM, and CatBoost models using `src/utils/hyperparameter_tuning.py`.
*   Train a stacking ensemble model using the best-performing base models.
*   Save the trained ensemble model as `stacking_ensemble_model.pkl` in the `models/` directory.
*   Generate `submission.csv` in the `data/` directory with calorie predictions for the test set.

### 2. Hyperparameter Tuning (Individual Models)

If you wish to perform hyperparameter tuning for individual models (LGBM, XGB, CatBoost) separately, you can refer to the `src/utils/hyperparameter_tuning.py` script. The original repository also mentioned a `.bat` file for this purpose, but for cross-platform compatibility, direct Python execution is recommended.

### 3. Creating Submissions from Pre-trained Models

The `notebooks/create_submissions.ipynb` notebook can be used to generate submission files from pre-trained models (e.g., `catboost_model.pkl`, `xgb_model.pkl`, `lgb_model.pkl`). This notebook also includes functionality to optimize weights for an ensemble of predictions from different models.

To use this notebook:

1.  Ensure you have the necessary `.pkl` model files in the `models/` directory (these would typically be generated by individual model training scripts or `train_model.py`).
2.  Open the notebook using Jupyter Lab or Jupyter Notebook:

    ```bash
    jupyter lab notebooks/create_submissions.ipynb
    ```

3.  Run the cells in the notebook to generate predictions and create the final `submission.csv`.

### 4. Data Visualization and Exploration

The `notebooks/Visualization.ipynb` notebook provides initial data exploration and visualization insights into the dataset. You can use this notebook to understand the data distributions, relationships between features, and potential outliers.

To explore the data:

```bash
jupyter lab notebooks/Visualization.ipynb
```




## Feature Engineering

Feature engineering plays a crucial role in improving the model's performance. The `src/utils/feature_engineering.py` script contains the `create_features` function, which generates a variety of new features from the raw input data. These features are designed to capture more complex relationships and patterns that might be indicative of calorie expenditure.

Key feature engineering techniques applied include:

*   **Ratio Features:** Creating ratios like `Weight_per_Age` and `HeartRate_per_kg` to normalize values and highlight relationships between different physiological measurements.
*   **Interaction Terms:** Generating interaction terms such as `Duration_heart_rate` (Duration * Heart Rate) to capture combined effects of activities and heart rate. Cross-terms between numerical features are also added.
*   **Body Mass Index (BMI):** Calculating BMI using `Weight` and `Height` to provide a standardized measure of body fat based on these two variables.
*   **Physiological Estimates:** Deriving estimated physiological metrics like `max_heart_rate`, `METs` (Metabolic Equivalents), and `VO2_max` (maximal oxygen uptake) based on age and heart rate, which are directly related to physical exertion and calorie burn.
*   **Activity Score:** A composite score (`activity_score`) combining `Duration`, `Heart_Rate`, and `Body_Temp` to quantify overall activity intensity.
*   **Sex-based Interactions:** Creating interaction terms between `Sex` and other features (`Duration`, `Heart_Rate`, `Body_Temp`) to account for potential gender-specific differences in calorie expenditure patterns.
*   **Age and Duration Specific Heart Rate/Body Temperature:** Generating features that capture `Heart_Rate` and `Body_Temp` specific to certain `Age` and `Duration` values, allowing the model to learn more granular patterns.

These engineered features provide a richer representation of the data, enabling the models to learn more effectively and make more accurate predictions.




## Modeling Approach

This project employs a robust modeling approach centered around ensemble learning and GPU-accelerated hyperparameter tuning to maximize prediction accuracy for calorie expenditure.

### Ensemble Learning (Stacking)

The core of the prediction system is a stacking ensemble model. Stacking involves training multiple diverse base models and then training a meta-model to make predictions based on the outputs of these base models. This approach leverages the strengths of individual models and combines them to achieve superior performance compared to any single model.

The base models used in this ensemble are:

*   **XGBoost (XGBRegressor):** A highly efficient and flexible gradient boosting library known for its speed and performance.
*   **LightGBM (LGBMRegressor):** Another gradient boosting framework that uses tree-based learning algorithms, designed for distributed and high-performance computing.
*   **CatBoost (CatBoostRegressor):** A gradient boosting library that handles categorical features automatically and is known for its robustness and high accuracy.

The meta-model, typically an `ElasticNet` regressor, learns to optimally combine the predictions from these base models. The ensemble process involves:

1.  **Out-of-Fold Predictions:** Each base model is trained on different folds of the training data, and predictions are generated for the unseen folds. These out-of-fold predictions serve as the training data for the meta-model.
2.  **Full Dataset Training:** Each base model is then trained on the entire training dataset to be used for making predictions on new, unseen data (the test set).
3.  **Meta-Model Training:** The meta-model is trained on the out-of-fold predictions of the base models, with the original target variable as its target.
4.  **Final Prediction:** For new data, each trained base model makes a prediction, and these predictions are fed into the meta-model, which then outputs the final ensemble prediction.

This multi-stage approach helps to reduce overfitting and improve the generalization capability of the overall model.

### GPU-Accelerated Hyperparameter Tuning

To find the optimal configurations for the base models, GPU-accelerated hyperparameter tuning is utilized. This significantly speeds up the search process for the best model parameters, which is crucial given the large search spaces and the computational intensity of training gradient boosting models.

*   **Randomized Search Cross-Validation:** Instead of an exhaustive grid search, `RandomizedSearchCV` is used to efficiently explore a wide range of hyperparameter combinations. This method samples a fixed number of parameter settings from specified distributions, offering a good balance between exploration and computational cost.
*   **GPU Utilization:** XGBoost, LightGBM, and CatBoost are configured to leverage GPU resources (`tree_method='gpu_hist'`, `device='gpu'`, `task_type='GPU'`). This enables faster model training and hyperparameter tuning, especially on large datasets.
*   **Memory Management:** The tuning process considers GPU memory usage, with parameters like `max_bin` for LightGBM and XGBoost, and `gpu_ram_part` for CatBoost, helping to manage memory efficiently and prevent out-of-memory errors.

The `src/utils/hyperparameter_tuning.py` script orchestrates this process, identifying the best hyperparameters for each base model based on the Root Mean Squared Logarithmic Error (RMSLE) metric, which is particularly suitable for targets with a skewed distribution like calorie expenditure.



# Reinforcement Learning Ensemble Model for Calorie Prediction

This project also attempts to implement a reinforcement learning-based ensemble method that intelligently selects between different machine learning models (CatBoost, XGBoost, and LightGBM) to make optimal predictions for calorie estimation.

## Overview

The system uses a Deep Q-Network (DQN) agent to learn which model performs best for different input features. Instead of simply averaging predictions from multiple models, the RL agent learns to dynamically select the most appropriate model based on the characteristics of each input sample.

## Architecture

### Key Components

1. **DQN (Deep Q-Network)**: A neural network that learns to map input features to model selection actions
2. **ModelSelectorAgent**: The reinforcement learning agent that manages model selection and training
3. **Ensemble Pipeline**: Complete workflow from data loading to final predictions

### Model Selection Strategy

- **State**: Feature vectors from the input data
- **Actions**: Choice between 3 models (CatBoost=0, XGBoost=1, LightGBM=2)
- **Reward**: Negative log error (better predictions = higher rewards)
- **Policy**: ε-greedy exploration with decaying epsilon

## Data Structure

The code expects the following CSV files:

### Training Data
- `data/train.csv` - Original training data with features and target 'Calories'
- `data/new3/catboost_train_pred.csv` - CatBoost predictions on training data
- `data/new3/xgb_train_pred.csv` - XGBoost predictions on training data
- `data/xgb_train_pred.csv` - LightGBM predictions on training data

### Test Data
- `data/test.csv` - Test features
- `data/new3/catboost_submission.csv` - CatBoost predictions on test data
- `data/new3/xgb_submission.csv` - XGBoost predictions on test data
- `data/xgb_submission.csv` - LightGBM predictions on test data

## Usage

### Basic Training and Prediction
First generate predictions for each model on the training data and save them as specified above. (modify datapath accordingly in the script if needed) 
Then run the training script: `src/rl_torch.py`. 

## Key Features

### GPU Support
- Automatically detects and uses GPU if available
- Falls back to CPU if GPU is not found
- Includes GPU performance validation

### Feature Engineering
- Uses `utils.feature_engineering.create_features()` for preprocessing
- Standardizes features using StandardScaler
- Handles feature normalization automatically

### Training Process
- **Episodes**: 150 training episodes
- **Batch Size**: 64 samples per batch
- **Exploration**: ε-greedy with decay (1.0 → 0.01)
- **Learning Rate**: 0.001
- **Discount Factor**: 0.95

### Model Architecture
```
Input Layer → 24 neurons (ReLU, Dropout 0.2) 
           → 24 neurons (ReLU, Dropout 0.2) 
           → 3 output neurons (model selection)
```

## Training Metrics

The system tracks several metrics during training:

- **RMSLE (Root Mean Square Log Error)**: Primary evaluation metric
- **R² Score**: Coefficient of determination
- **Average Reward**: RL training progress
- **Model Usage Statistics**: Distribution of model selections
- **Epsilon Decay**: Exploration vs exploitation balance

## Output Files

- `best_agent.pth` - Best performing model weights
- `rl_ensemble_submission.csv` - Final test predictions


### Reward Function
```python
error = abs(log1p(prediction) - log1p(true_value))
reward = -error
```

### Q-Learning Update
Uses standard DQN update rule with target network approach:
```
Q(s,a) = r + γ * max(Q(s',a'))
```